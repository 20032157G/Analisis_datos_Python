{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Análisis de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Datos\n",
    "\n",
    "En un mundo cada vez más centralizado en torno a la tecnología de la información, grandes cantidades de datos se producen y almacenan cada día. A menudo, estos datos proceden de sistemas de detección automática, sensores e instrumentación científica, o los producen diariamente e inconscientemente cada vez que realizan retiros del banco o realiza una compra  o incluso cuando se publican en redes sociales.\n",
    "\n",
    "Los datos en realidad no son información, al menos en términos de su forma. En un primer momento es dificil entender su esencia si no es estrictamente el número, la palabra o el tiempo que ellos reportan. La información es en realidad el resultado del procesamiento de un  cierto conjunto de datos, del que se extrae algunas conclusiones que se pueden utilizar de diversas maneras. Este proceso de extraer información de los datos (raw) es precisamente el **análisis de datos**.\n",
    "\n",
    "El propósito del análisis de datos es precisamente extraer información que no es fácilmente deducible pero que, cuando se entiende, conduce a la posibilidad de realizar estudios sobre los mecanismos de los sistemas que los han producido, permitiendo así la posibilidad de hacer previsiones de posibles respuestas de estos sistemas y su evolución en el tiempo.\n",
    "\n",
    "Partiendo de un enfoque metódico simple, el análisis de datos se ha convertido en una verdadera disciplina que conduce al desarrollo de metodologías reales generadoras de **modelos**. El modelo es de hecho la traducción en una forma matemática de un sistema puesto en estudio. Una vez que hay una forma matemática o lógica capaz de describir las respuestas del sistema bajo diferentes niveles de precisión, entonces puede hacer predicciones sobre su desarrollo o respuesta a ciertas entradas. Por lo tanto, el objetivo del análisis de datos no es el modelo, sino la calidad  de su **poder predictivo**.\n",
    "\n",
    "El poder predictivo de un modelo depende no sólo de la calidad de las técnicas de modelado, sino también de la capacidad de elegir un buen conjunto de datos sobre el cual construir el análisis completo de los datos. Por lo tanto, la **búsqueda de datos**, su **extracción** y su posterior **preparación**, al tiempo que representan las actividades preliminares de un análisis, también pertenecen al propio análisis de datos, debido a su importancia en el éxito de los resultados.\n",
    "\n",
    "Paralelamente a todas las etapas de procesamiento del análisis de datos, se han desarrollado diversos métodos de **visualización de datos**. De hecho, para entender los datos, tanto individualmente como en función del rol que juegan en el conjunto de datos, no hay mejor sistema que desarrollar  técnicas de representación gráfica capaces de transformar la información  a veces implícitamente escondida, en figuras. Durante los años se han desarrollado muchos modos de visualización para diferentes modos de visualización de datos: los **gráficas**.\n",
    "\n",
    "Al final del proceso del análisis de datos, tendremos un modelo y un conjunto de gráficas para luego  predecir las respuestas del sistema en estudio. Terminado eso, debemos empezar  la **fase de prueba**. El modelo se probará utilizando otro conjunto de datos para los que conocemos la respuesta del sistema. Sin embargo, estos datos no se utilizan para la definición del modelo predictivo. Dependiendo de la capacidad del modelo para replicar respuestas observadas reales, tendremos un cálculo de error y un conocimiento de la validez del modelo y sus límites operativos.  Una vez que haya evaluado esto, se puede pasar a la última fase del análisis de datos: el **despliegue**. Esto consiste en  la implementación de las decisiones que se tomarán sobre la base de las predicciones generadas por el modelo y los riesgos de que tal decisión también se prevea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominios de conocimiento para manejar  Análisis de Datos\n",
    "\n",
    "Análisis de datos es una disciplina para el estudio de problemas que pueden ocurrir en varios campos de aplicación y que  requieren un buen conocimiento de computación y de  conceptos matemáticos y estadísticos. Muchas de estas disciplinas son la base de los métodos de análisis de datos y su conocimiento de ellos es casi necesaria.\n",
    "\n",
    "###  Ciencia de la computación\n",
    "\n",
    "*  Todos los pasos relacionados con  análisis de datos implican el uso de software de cálculo (como IDL, Matlab, etc.) y lenguajes de programación (como C ++, Java, Python).\n",
    "\n",
    "* Conocimientos de formatos. Los datos están estructurados y almacenados en archivos o tablas de bases de datos con formatos particulares:  XML, JSON, o simplemente archivos XLS o CSV que son los formatos comunes para almacenar y recopilar datos  y muchas aplicaciones también permiten su lectura y gestión de datos almacenados en ellos.\n",
    "* Para la extracción de datos contenidos en una base de datos, es necesario conocer el lenguaje de consulta SQL o utilizar software  desarrollado para la extracción de datos de una base de datos determinada.\n",
    "* Para lgunos tipos específicos de datos, los datos no están disponibles en formato de *pretratado* y de forma  explícita, sino que están presentes en archivos de texto (documentos, archivos de registro) o en páginas web, mostrados como gráficos, medidas o tablas HTML que requieren conocimientos de parsing  y la eventual extracción de estos datos **(Web Scraping)**.\n",
    "\n",
    "### Matématica y Estadística\n",
    "\n",
    "Análisis de datos requiere mucha matemática, durante el tratamiento y procesamiento de datos. Una cierta familiaridad con los principales conceptos estadísticos también es necesaria porque todos los métodos que se aplican en el análisis y la interpretación de los datos se basan en estos conceptos. La estadística proporciona los conceptos que forman la base del análisis de datos. Entre las técnicas estadísticas más utilizadas en el análisis de datos tenemos\n",
    "\n",
    "* Métodos Bayesianos\n",
    "* Regresión\n",
    "* Clustering\n",
    "\n",
    "### Machine Learning y IA\n",
    "\n",
    "Machine Learning es una disciplina que hace uso de toda una serie de procedimientos y algoritmos que analizan los datos para reconocer patrones, hacer clasificaciones y tendencias para luego extraer información útil de una manera totalmente automatizada.\n",
    "\n",
    "Esta disciplina se está convirtiendo cada vez más en una herramienta fundamental del análisis de datos, por lo que su conocimiento, al menos de forma general, reviste una importancia fundamental para el analista de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python y Análisis de Datos\n",
    "\n",
    "[Python](https://www.python.org/) es un lenguaje de programación ampliamente utilizado debido a su gran número de bibliotecas que proporcionan un conjunto completo de herramientas para el análisis y la manipulación de datos.\n",
    "\n",
    "Python no sólo proporciona una plataforma para el procesamiento de datos, sino que también tiene algunas características que lo hacen único en comparación con otros lenguajes y aplicaciones especializadas. El desarrollo de un número cada vez mayor de bibliotecas científicas, la implementación de algoritmos con  metodologías más innovadoras y la capacidad de interactuar con otros lenguajes de programación (C y Fortran) hacen que Python sea especial.\n",
    "\n",
    "Además, Python no sólo está especializado para el análisis de datos, sino que también tiene muchas otras aplicaciones, como programación genérica, scripting, interfaz con bases de datos y  más recientemente, desarrollo web, gracias al frameworks web [Django](https://www.djangoproject.com/). Por lo tanto, es posible desarrollar proyectos de análisis de datos que sean totalmente compatibles con un servidor web con la posibilidad de integrarlo en la Web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas \n",
    "\n",
    "Algunas herramientas para utilizar en Análisis de Datos son:\n",
    "\n",
    "* [NumPy](http://www.numpy.org/), es la biblioteca natural para python numérico. La característica más potente de NumPy es la matriz n-dimensional. Esta biblioteca contiene funciones básicas de álgebra lineal, transformadas de Fourier, capacidades avanzadas de números aleatorios y herramientas para la integración con otros lenguajes de bajo nivel como Fortran, C y C ++.\n",
    "\n",
    "* [SciPy](https://www.scipy.org/) es la biblioeteca para python científico. SciPy se basa en NumPy y es una de las bibliotecas más útiles por la variedad de módulos de ciencia y ingeniería de alto nivel con la que cuenta, como la transformada discreta de Fourier, álgebra lineal, optimización, matrices dispersas, etc.\n",
    "\n",
    "* [Matplotlib]() es una librería de Python para crear una gran variedad de gráficos, a partir de histogramas, lineas, etc, usando si es necesario comandos de látex para agregar matemáticas a los gráficos.\n",
    "\n",
    "* [Pandas](http://pandas.pydata.org/) es una librería para operaciones y manipulaciones de datos estructurados. Pandas ha sido añadido recientemente a Python y han sido fundamental para impulsar el uso de Python en la ciencia de datos.\n",
    "\n",
    "* [Scikit Learn](http://scikit-learn.org/stable/), es tal vez la mejor biblioteca para Machine Learning, construida sobre NumPy, SciPy y Matplotlib, esta biblioteca contiene una gran cantidad de herramientas eficientes para el Machine Learning y el modelado estadístico incluyendo clasificación, regresión, agrupación y reducción de la dimensionalidad.\n",
    "\n",
    "* [Statsmodels](http://statsmodels.sourceforge.net/) es una biblioteca para el modelado estadístico. Statsmodels es un módulo de Python que permite a los usuarios explorar datos, estimar modelos estadísticos y realizar pruebas estadísticas.\n",
    "\n",
    "* [Seaborn](https://seaborn.pydata.org/) es una libreria para la visualización de datos estadísticos. Seaborn es una biblioteca para hacer atractivos e informativos los gráficos estadísticos en Python. Se basa en matplotlib. Seaborn pretende hacer de la visualización una parte central de la exploración y la comprensión de los datos.\n",
    "\n",
    "* [Bokeh](http://bokeh.pydata.org/en/latest/) es una biblioteca para crear gráficos interactivos y aplicaciones de datos en modernos navegadores web. Permite al usuario generar gráficos elegantes y concisos al estilo de D3.js. Además, tiene la capacidad de interactividad de alto rendimiento en conjuntos de datos muy grandes o en streaming.\n",
    "\n",
    "* [Blaze](http://blaze.pydata.org/) son librerias que sirve para ampliar la capacidad de Numpy y Pandas a conjuntos de datos distribuidos y en streaming. Puede utilizarse para acceder a datos de una multitud de fuentes, incluyendo Bcolz, MongoDB, SQLAlchemy, Apache Spark, PyTables, etc. Junto con Bokeh, Blaze puede actuar como una herramienta muy potente para crear visualizaciones y dashboards eficaces con los datos.\n",
    "\n",
    "* [Scrapy](https://scrapy.org/), es un framework muy útil para obtener patrones específicos de datos. Tiene la capacidad de comenzar en una URL de sitio web y luego buscar a través de las páginas web del sitio web para recopilar información.\n",
    "\n",
    "* [SymPy](https://simpy.readthedocs.io/en/latest/) es una biblioteca para la computación simbólica. Tiene amplias capacidades desde la aritmética simbólica básica hasta el cálculo, el álgebra, la matemática discreta y la física cuántica. Otra característica útil es la capacidad de formatear el resultado de los cálculos como código LaTeX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Datos , información y conocimiento\n",
    "\n",
    "Los datos son los eventos registrados en el mundo. Cualquier cosa que pueda ser medida o categorizada puede ser convertida en datos. Una vez recolectados, estos datos pueden ser estudiados y analizados tanto para comprender la naturaleza de los eventos como muy a menudo también para hacer predicciones o al menos para tomar decisiones informadas.\n",
    "\n",
    "Se puede hablar de conocimiento cuando la información se convierte en un conjunto de reglas que  ayudan a comprender mejor ciertos mecanismos y por lo tanto, hacer predicciones sobre la evolución de algunos eventos.\n",
    "\n",
    "Los datos pueden ser divididos  en distintas categorias:\n",
    "\n",
    "* Categóricos\n",
    "    - nominal\n",
    "    - ordinal\n",
    "    \n",
    "* Numérico\n",
    "    - discreto\n",
    "    - continuo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los **datos categóricos** son valores o observaciones que pueden dividirse en grupos o categorías. Hay dos tipos de valores categóricos: **nominal** y **ordinal**. Una variable nominal no tiene orden intrínseco que se identifica en su categoría. En cambio, una variable ordinal tiene un orden predeterminado.\n",
    "\n",
    "Los **datos numéricos** son valores u observaciones que provienen de mediciones. Existen dos tipos de valores numéricos diferentes: números **discretos** y **continuos**. Los valores discretos son valores que pueden contarse y que son distintos y separados entre sí. Los valores continuos, por otra parte, son valores producidos por mediciones u observaciones que asumen cualquier valor dentro de un rango definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El proceso del Análisis de Datos\n",
    "\n",
    "\n",
    "El análisis de datos puede ser descrito como un proceso que consiste en varios pasos en los cuales los datos(raw) son transformados y procesados para producir visualizaciones de datos y pueden hacer predicciones gracias a un modelo matemático basado en los datos recopilados.\n",
    "\n",
    "Así el **análisis de datos no es más que una secuencia de pasos, cada uno de los cuales juega un papel clave en los posteriores** . Por lo tanto, el análisis de datos está casi esquematizado como una cadena de proceso que consiste en la siguiente secuencia de etapas:\n",
    "\n",
    "* Definición del problema\n",
    "* Extracción de datos \n",
    "* Limpieza de datos\n",
    "* Transformación de datos \n",
    "* Exploración de datos\n",
    "* Modelado predictivo\n",
    "* Modelo de validación/Prueba\n",
    "* Visualización y interpretacion de resultados\n",
    "* Despliege de la solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición del problema\n",
    "\n",
    "El proceso de análisis de datos en realidad comienza mucho antes de la recopilación de datos (raw). De hecho un análisis de datos siempre comienza con un problema a resolver, que debe definirse. El problema se define sólo después de haber centrado bien el sistema que desea estudiar: esto puede ser un mecanismo, una aplicación o un proceso en general. Este estudio puede ser para comprender mejor su funcionamiento, pero en particular el estudio se diseñará para comprender los principios de su comportamiento, para poder hacer predicciones o tomar decisiones (definidas como una elección informada).\n",
    "\n",
    "Una vez que el problema se ha definido y documentado, puede pasar a la **planificación del proyecto** de un análisis de datos. La planificación es necesaria para entender qué profesionales y recursos son necesarios para cumplir con los requisitos para llevar a cabo el proyecto de la manera más eficiente posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de datos\n",
    "\n",
    "Una vez que se ha definido el problema, el primer paso es obtener los datos para realizar el análisis. Los datos deben ser elegidos con el propósito básico de construir el modelo predictivo, y por lo tanto su selección es crucial para el éxito del análisis. Los datos de muestra recogidos deben reflejar lo más posible las\n",
    "Mundo, Una mala elección de los datos, o incluso la realización de análisis en un conjunto de datos que no es perfectamente representativo del sistema, dará lugar a modelos que se alejan del sistema en estudio.\n",
    "\n",
    "La búsqueda y recuperación de datos a menudo requieren una forma de intuición que va más allá de la mera investigación técnica y extracción de datos. También requiere una cuidadosa comprensión de la naturaleza de los datos y su forma, que sólo la buena experiencia y el conocimiento en el campo de aplicación del problema puede dar. Independientemente de la calidad y la cantidad de datos necesarios, otro aspecto es la búsqueda y la elección correcta de las **fuentes de datos**.\n",
    "\n",
    "Cuando desea obtener  datos, un buen lugar para comenzar es  la web. Pero la mayoría de los datos en la web pueden ser difíciles de capturar; de hecho, no todos los datos están disponibles en un archivo o base de datos, pero pueden ser más o menos implícitamente contenidos  dentro de una  páginas html  en muchos formatos diferentes.\n",
    "\n",
    "Para ello, se ha desarrollado una metodología llamada **Web Scraping**, que permite recopilar datos mediante el reconocimiento de la presencia específica de etiquetas HTML dentro de las páginas web. Hay software diseñado específicamente para este propósito, y una vez que se encuentra una ocurrencia, extraen los datos deseados. Una vez completada la búsqueda, obtendrá una lista de datos listos para ser usados en  el análisis de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de datos\n",
    "\n",
    "Entre todos los pasos involucrados en el análisis de datos, la preparación de datos, aunque aparentemente menos problemática, es en realidad una de las requiere más recursos y más tiempo para ser completada. Los datos recopilados se recogen a menudo de diferentes fuentes de datos, cada uno de los cuales tendrá datos con una representación diferente y formato.\n",
    "\n",
    "Por lo tanto, todos estos datos tendrán que ser preparados para el proceso de análisis de datos. La preparación de los datos se refiere a la obtención, limpieza, normalización y transformación de datos en un conjunto de datos optimizado, es decir, en un formato preparado, normalmente tabular, adecuado para los métodos de análisis que se han programado durante la fase de diseño.\n",
    "\n",
    "Muchos son los problemas que deben evitarse, como valores no válidos, ambiguos o que faltan, campos replicados o datos fuera de rango, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de datos / Visualización\n",
    "\n",
    "La exploración de  datos es esencialmente la búsqueda de datos en una presentación gráfica o estadística con el fin de encontrar patrones, conexiones y relaciones en los datos. La visualización de datos es la mejor herramienta para resaltar patrones posibles.\n",
    "\n",
    "La exploración de datos consiste en un examen preliminar de los datos, que es importante para comprender el tipo de información que se ha recopilado y lo que significa. En combinación con la información adquirida durante la definición  del problema, esta categorización determinará qué método de análisis de datos será más adecuado para llegar a una definición de modelo.\n",
    "\n",
    "Generalmente, esta fase, además de un estudio detallado de gráficos a través de los datos de visualización, puede consistir en una o más de las siguientes actividades:\n",
    "\n",
    "* Resumen de datos\n",
    "* Agrupación de datos\n",
    "* Exploración de la relación entre los distintos atributos\n",
    "* Identificación de patrones y tendencias\n",
    "* Construcción de modelos de regresión\n",
    "* Construcción de modelos de clasificación\n",
    "\n",
    "\n",
    "Generalmente, el análisis de datos requiere procesos de resumen de declaraciones con respecto a los datos a estudiar. El **resumen** es un proceso por el cual los datos se reducen a la interpretación sin sacrificar información importante.\n",
    "\n",
    "Clustering es un método de análisis de datos que se utiliza para encontrar grupos unidos por atributos comunes (**agrupación**). \n",
    "\n",
    "Otro paso importante del análisis se centra en la identificación de relaciones, tendencias y anomalías en los datos. Con el fin de descubrir este tipo de información, uno a menudo tiene que recurrir a las herramientas, así como realizar otra ronda de análisis de datos, esta vez en la visualización de datos en sí.\n",
    "\n",
    "Otros métodos de data mining, como árboles de decisión y reglas de asociación, extraen automáticamente información  importantes de los datos. Estos enfoques se pueden utilizar en paralelo con la visualización de datos para encontrar información sobre las relaciones entre los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelado predictivo\n",
    "\n",
    "El modelado predictivo es un proceso utilizado en el análisis de datos para crear o elegir un modelo estadístico adecuado para predecir la probabilidad de un resultado.\n",
    "\n",
    "Después de explorar los datos se tiene toda la información necesaria para desarrollar el modelo matemático que codifica la relación entre los datos. Estos modelos son útiles para la comprensión del sistema en estudio, y de una manera específica se utilizan para dos propósitos principales. El primero  es hacer predicciones sobre los valores de datos producidos por el sistema; en este caso, trataremos con **modelos de regresión**. El segundo es clasificar los nuevos  de datos, y en este caso, se utilizará **modelos de clasificación** o **modelos de agrupación**. De hecho, es posible dividir los modelos según el tipo de resultado que produzcan:\n",
    "\n",
    "* Modelos de clasificación: Si el resultado obtenido por el tipo de modelo es categórico.\n",
    "* Modelos de regresión: Si el resultado obtenido por el tipo de modelo es numérico.\n",
    "* Modelos de agrupación: Si el resultado obtenido por el tipo de modelo es descriptivo.\n",
    "\n",
    "\n",
    "Los métodos simples para generar estos modelos incluyen técnicas tales como regresión lineal, regresión logística, árboles de clasificación y regresión, y k-vecinos más cercanos. Pero los métodos de análisis son numerosos, y cada uno tiene características específicas que lo hacen excelente para algunos tipos de datos y análisis.\n",
    "\n",
    "Cada uno de estos métodos producirá un modelo específico, y luego su elección es relevante para la naturaleza del modelo del producto. Algunos de estos modelos aportarán valores correspondientes al sistema real, y también según su estructura explicarán algunas características del sistema estudiado de una manera simple y clara. Otros modelos seguirán dando buenas predicciones, pero su estructura no será más que una \"caja negra\" con capacidad limitada de explicar algunas características del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación de modelo\n",
    "\n",
    "La validación del modelo, es decir, la fase de prueba, es una fase importante que nos permite validar el modelo construido sobre la base de los datos de inicio. Esto es importante porque permite evaluar la validez de los datos producidos por el modelo comparándolos directamente con el sistema actual. Pero esta vez, estamos saliendo del conjunto de datos iniciales sobre los cuales se ha establecido todo el análisis.\n",
    "\n",
    "Generalmente, nos referimos a los datos como el **conjunto de entrenamiento**, cuando se utilizan para la construcción del modelo, y como ** conjunto de validación**, cuando se utilizan para validar el modelo. Por lo tanto, al comparar los datos producidos por el modelo con los producidos por el sistema, se  podrá evaluar el error y utilizando diferentes conjuntos de datos de prueba, puede estimar los límites de validez del modelo generado. De hecho, los valores correctamente predichos podrían ser válidos sólo dentro de un cierto rango, o tener diferentes niveles de coincidencia dependiendo del rango de valores tomado en cuenta.\n",
    "\n",
    "Este proceso le permite no sólo evaluar numéricamente la efectividad del modelo, sino también compararlo con cualquier otro modelo existente. Hay varias técnicas a este respecto; la más famosa es la **validación cruzada**. Esta técnica se basa en la división del conjunto de entrenamiento en diferentes partes.  Cada una de estas partes, a su vez, se utilizará como el conjunto de validación y cualquier otro como el conjunto de entrenamiento. De esta manera iterativa, tendremos un modelo cada vez más perfeccionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despliege\n",
    "\n",
    "Este es el último paso del proceso de análisis, que pretende presentar los resultados, es decir, las conclusiones del análisis. En el proceso de implementación, en el entorno empresarial, el análisis se traduce en un beneficio para el cliente que lo ha encargado. En entornos técnicos o científicos, se traduce en soluciones de diseño o publicaciones científicas. Es decir, el despliegue consiste básicamente en poner en práctica los resultados obtenidos del análisis de los datos.\n",
    "\n",
    "Hay varias maneras de implementar los resultados de un análisis de datos o data mining. Normalmente, el despliegue de un analista de datos consiste en escribir un informe para la administración o para el cliente que solicitó el análisis. Este documento describirá conceptualmente los resultados obtenidos del análisis de los datos. El informe debe dirigirse a los gerentes, que son capaces de tomar decisiones. Entonces ellos realmente pondrán en práctica las conclusiones del análisis.\n",
    "\n",
    "En la documentación suministrada por el analista, cada uno de estos cuatro temas generalmente se discutirán en detalle:\n",
    "\n",
    "* Resultados de analisis\n",
    "* Implementación de la decisión\n",
    "* Análisis de riesgo\n",
    "* Medición del impacto en los negocios\n",
    "\n",
    "Cuando los resultados del proyecto incluyen la generación de modelos predictivos, estos modelos se pueden implementar como una aplicación independiente o pueden integrarse en otro software.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un ejemplo\n",
    "\n",
    "Si buscando un trabajo en data science, hemos *scraped* (tomado de la Web) 1.000 descripciones de trabajo para las empresas que contratan activamente a científicos de datos (a partir de enero de 2016). El objetivo aquí es mirar algunas de las palabras clave más comunes que las personas usan en sus descripciones de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\C-LaraAvila\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\C-LaraAvila\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9594\n",
      "data 1275\n",
      "learning 331\n",
      "machine 321\n",
      "machine learning 301\n",
      "analytics 283\n",
      "experience 241\n",
      "scientist 218\n",
      "data scientist 197\n",
      "scientists 180\n",
      "mining 166\n",
      "statistical 149\n",
      "data mining 137\n",
      "analysis 135\n",
      "science 126\n",
      "team 120\n",
      "research 118\n",
      "data scientists 113\n",
      "business 99\n",
      "years 89\n",
      "techniques 86\n",
      "work 86\n",
      "modeling 83\n",
      "statistics 82\n",
      "health 80\n",
      "using 78\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "textos = [] # Obtener publicaciones de la web\n",
    "for i in range(0,1000,10): # Ciclo en de 100 paginas  de trabajo \n",
    "    soup = BeautifulSoup(requests.get('http://www.indeed.com/jobs?q=data+scientist&start='+str(i)).text)\n",
    "    textos += [a.text for a in soup.findAll('span', {'class':'summary'})]\n",
    "\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "# Hacemos  un contador vectorial   para obtener recuentos básicos\n",
    "\n",
    "matriz = vect.fit_transform(textos)\n",
    "# fijamos  y aprendemos el vocabulario \n",
    "\n",
    "print (len(vect.get_feature_names()))  # numero de caracteristicas hay..\n",
    "\n",
    "freqs = [(word, matriz.getcol(idx).sum()) for word, idx in vect.vocabulary_.items()]\n",
    "#ordenamos de menor a mayor\n",
    "for frase, veces in sorted (freqs, key = lambda x: -x[1])[:25]:\n",
    "    print (frase, veces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecturas\n",
    "\n",
    "* [What is a data scientist?](https://www.quora.com/What-is-a-data-scientist-3?redirected_qid=796075).\n",
    "* Introducing Data Science David Cielen, Mohamed Ali, Arno D.B Meysman Manning Publications 2016 Chapter 1, 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
